---
title: "work-first"
author: "Alona Muzikansky"
date: "5/8/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(broom)
library(dplyr)
library(ggplot2)
library(glmnet)
library(gridExtra)
library(kableExtra)
library(stepPlr)
library(knitr)
library(ROCR)
#library(sAIC)
```



```{r}
MI <- read.csv("data/Myocardial infarction complications Database.csv")
MI <- MI[,c(2:112, 121)]
dim(MI) # 1700, 112
# variables with more than 10% missing values are excluded from the analysis
NApercent <- colSums(is.na(MI)) / nrow(MI)
selectcol <- NApercent[NApercent < 0.1]
MI.work <- MI[, names(selectcol)]
dim(MI.work) # 1700 112
# dimension of the dataset used in the analysis
MI.work <- na.omit(MI.work)
dim(MI.work) # 1074 95
```

```{r}
# transfer variable IBS_POST 
MI.work$IBS_POST <- ifelse(MI.work$IBS_POST == 0, 0, 1)
# change ZSN_A to character variable and relevel with 0 being the reference group
MI.work$ZSN_A <- as.character(MI.work$ZSN_A)
dim(MI.work)
```


### descriptive analysis of demographic information
```{r}
mean(MI.work$AGE) # 60.73
median(MI.work$AGE) # 62
age.hist <- ggplot(MI.work, aes(MI.work$AGE)) + geom_histogram() + 
  labs(x = "age", y = "count") + 
  geom_vline(xintercept = mean(MI.work$AGE), color = "blue") + 
  geom_vline(xintercept = median(MI.work$AGE), linetype = "dotted") +
  annotate("text", x = 40, y = 120, label = "mean = 60.73") +
  annotate("text", x = 40, y = 110, label = "median = 62") 
table(MI.work$SEX)
sex.plot <- ggplot(MI.work, aes(as.factor(MI.work$SEX))) + geom_bar() + 
  labs(x = "sex", y = "") + 
  scale_x_discrete(labels = c("female", "male")) +
  annotate("text", x = 1, y = 780, label = "female = 380") +
  annotate("text", x = 1, y = 720, label = "male = 649") 
table(MI.work$ZSN)
chf.plot <- ggplot(MI.work, aes(as.factor(MI.work$ZSN))) + geom_bar() + 
  labs(x = "CHF", y = "") + 
  scale_x_discrete(labels = c("no", "yes")) +
  annotate("text", x = 2, y = 780, label = "no = 806") +
  annotate("text", x = 2, y = 720, label = "yes = 268") 
grid.arrange(age.hist, sex.plot, chf.plot, nrow = 1)
```

```{r include=FALSE}
prescreen <- function(traindata){
  tokeep <- c()
  for (i in 1:94){
    uni <- glm(traindata$ZSN~traindata[,i],family = binomial)
    if(dim(summary(uni)$coef)[1]>1 & summary(uni)$coefficient[,4][2]<0.2){
      tokeep <- c(tokeep,i)
    }
  }
  screened <- cbind(ZSN=traindata$ZSN, data.frame(traindata[,tokeep]))
  return(screened)
}
```

### Regular GLM with subset seelction (Alona)

```{r}
# choice model: subset of predictors at the time of admission
mi.sub <- select(MI.work, -c(R_AB_1_n, R_AB_2_n, R_AB_3_n, NA_R_1_n, NA_R_2_n, NA_R_3_n, NOT_NA_1_n, NOT_NA_2_n, NOT_NA_3_n))
dim(mi.sub)
#choice model:
choice <- MI.work %>% 
  select(ZSN, AGE, SEX, IBS_POST, endocr_01, endocr_02, K_SH_POST, ritm_ecg_p_01, 
         TIME_B_S, L_BLOOD)
dim(choice)
```

## Model fitting

```{r warning=FALSE cache=TRUE}
#Backward selection - on the entire set.
glm.fit <-  glm(ZSN ~., data = MI.work, family = "binomial") 
#glm.fit <-  glm(ZSN ~., data = mi.sub, family = "binomial") 
best.fit.b <- step(glm.fit, scope=list(lower = ~ 1, upper = formula(glm.fit)), 
                   trace =F, direction = "backward")
best.fit.f <- step(glm.fit, scope=list(lower = ~ 1, upper = formula(glm.fit)),
                   trace =F, direction = "forward")
# backwardset <- MI.work %>% 
#   select(ZSN,AGE,STENOK_AN,FK_STENOK,ZSN_A,np_08,np_09,
#          endocr_01,zab_leg_01,FIB_G_POST,ant_im,lat_im,
#          inf_im,ritm_ecg_p_02,ritm_ecg_p_04,n_r_ecg_p_02,
#          n_p_ecg_p_04,n_p_ecg_p_06,fibr_ter_03,fibr_ter_06,
#          TIME_B_S,NA_R_2_n,NOT_NA_1_n,GEPAR_S_n,
#          TIKL_S_n)
backwardset <- MI.work %>% 
  select(AGE,STENOK_AN,FK_STENOK,GB,ZSN_A,
         np_08,np_09,np_10,endocr_01,zab_leg_01,
         ant_im,lat_im,ritm_ecg_p_02,ritm_ecg_p_04,
         n_r_ecg_p_02,n_p_ecg_p_04,n_p_ecg_p_06,fibr_ter_03,
         fibr_ter_06,NA_R_2_n,NOT_NA_1_n,B_BLOK_S_n,TIKL_S_n,ZSN)
```

```{r warning=FALSE}
# backwared selected set
glm.fitb <-  glm(ZSN ~., data = backwardset, family = binomial) 
predf <- predict(glm.fitb, newdata=backwardset, type="response")
summary(glm.fitb)$aic
# 998.6967
p.value=1-pchisq(deviance(glm.fitb),glm.fitb$df.residual)
# 0.9335986
```


```{r warning=FALSE}
#Backward selection - on the pre-screened dataset.
screened <- prescreen(MI.work)
glm.fit <-  glm(ZSN ~., data = screened, family = "binomial") 
#glm.fit <-  glm(ZSN ~., data = mi.sub, family = "binomial") 
best.fit.b <- step(glm.fit, scope=list(lower = ~ 1, upper = formula(glm.fit)), 
                   trace =F, direction = "backward")
best.fit.f <- step(glm.fit, scope=list(lower = ~ 1, upper = formula(glm.fit)),
                   trace =F, direction = "forward")
backwardset.screened <- MI.work %>% 
  select(AGE,GB,ZSN_A,endocr_01,zab_leg_01,ant_im,
         lat_im,ritm_ecg_p_02,fibr_ter_06,TIME_B_S,NA_R_2_n,
         NOT_NA_1_n,B_BLOK_S_n,TIKL_S_n,ZSN)
```

```{r warning=FALSE}
# backwared selected set with pre-screen
glm.fitb.s <-  glm(ZSN ~., data = backwardset.screened, family = binomial) 
predf <- predict(glm.fitb.s, newdata=backwardset.screened, type="response")
summary(glm.fitb.s)$aic
# 1010.444
p.value=1-pchisq(deviance(glm.fitb.s),glm.fitb.s$df.residual)
# 0.964
```

```{r warning=FALSE}
# full model
glm.fitf <-  glm(ZSN ~., data = MI.work, family = binomial) 
predf <- predict(glm.fitf, newdata=MI.work, type="response")
summary(glm.fitf)$aic
# 1100.243
p.value=1-pchisq(deviance(glm.fitf),glm.fitf$df.residual)
# 0.9335986
```


```{r warning=FALSE}
# pre-screen univariate selection
tokeep <- c()
  for (i in 1:94){
    uni <- glm(ZSN~ MI.work[,i],  data=MI.work, family = binomial)
    if(dim(summary(uni)$coef)[1]>1 & summary(uni)$coefficient[,4][2]<0.2){
      tokeep <- c(tokeep,i)
      #print(summary(uni)$coefficient[,4][2])
    }
  }
screened <- cbind(ZSN=MI.work$ZSN, data.frame(MI.work[,tokeep]))
glm.fit.screen <- glm(ZSN~., data=screened, family=binomial)  
predsc <- predict(glm.fit.screen, type="response")
summary(glm.fit.screen)$aic
# 1036.239
p.value=1-pchisq(deviance(glm.fit.screen),glm.fit.screen$df.residual)
# 0.9499305
```


```{r warning=FALSE}
# choice model - clinically driven - up to admission vars
glm.fitcc <-  glm(ZSN ~., data =mi.sub, family = binomial) 
predcc <- predict(glm.fitcc, type="response")
summary(glm.fitcc)$aic
# 1095.763
p.value=1-pchisq(deviance(glm.fitcc),glm.fitcc$df.residual)
# 0.917714
```


```{r warning=FALSE}
# choice model - Investigator discretion
glm.fitcr <-  glm(ZSN ~., data =choice, family = binomial) 
predcr <- predict(glm.fitcr, type="response")
summary(glm.fitcr)$aic
# 1167.811
p.value=1-pchisq(deviance(glm.fitcr),glm.fitcr$df.residual)
# 0.03708439
```

## model peformance - table 1
```{r}
predfull <- prediction(fitted(glm.fitf), MI.work$ZSN)
stats1 <- performance(predfull, 'tpr', 'fpr')
auc1 <- performance(predfull, measure = "auc")
auc1@y.values[[1]]

predcc <- prediction(fitted(glm.fitcc), mi.sub$ZSN)
stats2 <- performance(predcc, 'tpr', 'fpr')
auc2 <- performance(predcc,  measure = "auc")
auc2@y.values[[1]]

predcr <- prediction(fitted(glm.fitcr), choice$ZSN)
stats3 <- performance(predcr, 'tpr', 'fpr')
auc3 <- performance(predcr, measure = "auc")
auc3@y.values[[1]]

predscreen <- prediction(fitted(glm.fit.screen), screened$ZSN)
stats4 <- performance(predscreen, 'tpr', 'fpr')
auc4 <- performance(predscreen, measure = "auc")
auc4@y.values[[1]]

mod1.lab <- expression('Full model')
mod2.lab <- expression('Clinical set choice')
mod3.lab <- expression('Our choice')
mod4.lab <- expression('Univariate Pre-screen')

plot(stats1@x.values[[1]], stats1@y.values[[1]], type='s', 
     ylab=stats1@y.name, xlab=stats1@x.name, col='red', lwd=1, lty=1)
lines(stats2@x.values[[1]], stats2@y.values[[1]], type='s', col='green')
lines(stats3@x.values[[1]], stats3@y.values[[1]], type='s', col='orange')
lines(stats4@x.values[[1]], stats4@y.values[[1]], type='s', col='black')
abline(0,1, col='gray')
legend('bottomright', c(mod1.lab, mod2.lab, mod3.lab, mod4.lab),
col=c('red','green','orange','black'), 
       lwd=1, lty=1, cex=.9, bty='n')
```

```{r}
predbackward <- prediction(fitted(glm.fitb), backwardset$ZSN)
stats5 <- performance(predbackward, 'tpr', 'fpr')
auc5 <- performance(predbackward, measure = "auc")
auc5@y.values[[1]]

predbackward.s <- prediction(fitted(glm.fitb.s), backwardset.screened$ZSN)
stats6 <- performance(predbackward.s, 'tpr', 'fpr')
auc6 <- performance(predbackward.s, measure = "auc")
auc6@y.values[[1]]

mod5.lab <- expression('Backward selection')
mod6.lab <- expression('Backward selection + Pre-screen')
```

## model validation 

```{r include=FALSE}
cv.error <- function(data, prescreen=NULL){
  
  k=nrow(data)
  pred.train=rep(0,k)
  error.test=rep(0,k)
  pred=rep(0,k)
  train.error=rep(0,k)
  pred.test=rep(0,k)
  probs.test=rep(0,k)
  
  for(j in 1:k){
    traindata <- data[-j,]
    testdata <- data[j,]
    auc.all <- c()
    if (is.null(prescreen)){
      glm.fit <-  glm(ZSN ~., data = traindata, family = binomial) 
    } else {
      pres.data <- prescreen(traindata)
      glm.fit <-  glm(ZSN ~., data = pres.data, family = binomial) 
    }
    probt <- predict(glm.fit, type="response")
    predt <- rep(0,k)
    predt[probt>0.5]=1
    pred.train[j] <- mean(predt!= traindata$ZSN) 
    probs.test[j] <- predict(glm.fit, newdata = testdata, type="response")
    pred.test[j] <- (probs.test[j]>.5)
    error.test[j] <- (pred.test[j]!=testdata$ZSN)
    auc <- performance(prediction(fitted(glm.fit), traindata$ZSN), measure = "auc")
    auc.all <- c(auc.all, auc@y.values[[1]])
  }
  res <- return(c(mean(error.test), mean(pred.train), mean(auc.all)))
}
```


```{r warning=FALSE}
# LOOCV for the full model
cv.error(MI.work)
# 0.2318436 0.1883966 0.8161491
err.full <- c(0.2318436,0.1883966,0.8161491)
```

```{r warning=FALSE}
# LOOCV for full model with univariate feature screen selection:
cv.error(MI.work, prescreen)
# 0.2150838 0.2022175 0.7911514
err.screen <- c(0.2150838,0.2022175,0.7911514)
```


```{r warning=FALSE}
# LOOCV for the clinical choice subset
cv.error(mi.sub)
# 0.2188082 0.1937084 0.8051683
err.cc <- c(0.2188082,0.1937084,0.8051683)
```


```{r warning=FALSE}
# LOOCV for the backward selection subset
cv.error(backwardset)
# 0.2085661 0.1957969 0.8001530
err.backward <- c(0.2085661,0.1957969,0.8001530)
```

```{r warning=FALSE}
# LOOCV for the backward selection subset
cv.error(backwardset.screened)
# 0.2085661 0.1969274 0.7844234
err.backward.screen <- c(0.2085661,0.1969274,0.7844234)
```

```{r warning=FALSE}
# LOOCV for the backward selection subset
#cv.error(MI.work, prescreen, backward)
# this will take approximately 35 hours to finish
```


```{r warning=FALSE}
# LOOCV for choice model
cv.error(choice)
# 0.2476723 0.2446050 0.6482015
err.cr <- c(0.2476723,0.2446050,0.6482015)
```



```{r}
#prepare data for test error plot 
library(ggplot2)
 
ggplot(testerror, aes(x=n, y=testerr)) +
  geom_point(size=2) + 
  labs(x="Number of predictors",
       y="Test (Prediction) error rate")+
  geom_text(label=testerror$mtype, check_overlap = T, nudge_x = 3, vjust=1.3)
  
ggplot(testerror, aes(x=n, y=train.err)) +
  geom_point(size=2, col="red") + 
  labs(x="Number of predictors",
       y="Training (Prediction) error rate")+
  geom_text(label=testerror$mtype, check_overlap = T, nudge_x = 3, vjust=1.3)+
  coord_cartesian(ylim = c(0.2, 0.27))
```

```{r echo=FALSE}
models.u.table <- data.frame(type=c("Investigators' choice","Univariate Pre-screen",
                                    "Clinical set choice","Full"),
                              p=c(9,32,84,94),
                              AIC=c(1167.811,1036.239,1095.763,1100.243), 
                              pval=c(0.037,0.95,0.91,0.93))
kable(models.u.table, caption="Uregularized Logistic regression models",
      col.names=c("Model type","# predictors","AIC","p-value"))
```


```{r echo=FALSE}
models.r.table <- data.frame(type=c("Backward selection","Backward + Univariate Pre-screen",
                                    "Lasso","Lasso + Univariate Pre-screen",
                                    "Ridge","Ridge + Univariate Pre-screen"),
                              p=c(23,14,24,24,88,32),
                              AIC=c(998.6967,1010.444,1031.471,1022.898,NA,1042.574), 
                              pval=c(0.98,0.96,0,0,0,0))
kable(models.r.table, caption="Regularized Regression models",
      col.names=c("Model type","# predictors","AIC","p-value"))
```


```{r echo=FALSE}
errortable <- data.frame(err.cr=c(9,0.248,0.245,0.648),
                         err.screen=c(32,0.215,0.202,0.791),
                         err.cc=c(84,0.219,0.194,0.805),
                         err.full=c(94,0.232,0.188,0.816),
                         err.backward=c(23,0.209,0.196,0.800),
                         err.backward.screen=c(14,0.209,0.197,0.784),
                         err.lasso=c(24,0.201,0.197,0.779),
                         err.lasso.screen=c(24,0.266,0.237,0.760),
                         err.ridge=c(88,0.221,0.199,0.807),
                         err.ridge.screen=c(32,0.249,0.253,0.703)
                         )
rownames(errortable) <- c("# Predictors","Test error","Training error", "AUC")
colnames(errortable) <- c("Investigator's choice","Univariate pre-screen",
                          "Clinical Choice","Full model",
                          "Backward Selection","Backward Selection + pre-screen",
                          "Lasso","Lasso + pre-screen",
                          "Ridge Selection","Ridge + pre-screen")
mat <- as.matrix(errortable)
mat.t <- t(mat)
kable(mat.t, caption="Models' performance - LOOCV step")
# 
# library(ggplot2)
#  
# ggplot(testerror, aes(x=n, y=testerr)) +
#   geom_point(size=2) + 
#   labs(x="Number of predictors",
#        y="Test (Prediction) error rate")+
#   geom_text(label=testerror$mtype, check_overlap = T, nudge_x = 3, vjust=1.3)
#   
# 
# ggplot(testerror, aes(x=n, y=train.err)) +
#   geom_point(size=2, col="red") + 
#   labs(x="Number of predictors",
#        y="Training (Prediction) error rate")+
#   geom_text(label=testerror$mtype, check_overlap = T, nudge_x = 3, vjust=1.3)+
#   coord_cartesian(ylim = c(0.2, 0.27))
```



### Penalized GLM
lasso 
```{r}
# select best lambda via LOOCV 
set.seed(1)
x <- model.matrix(ZSN ~., MI.work)[, -95]
y <- MI.work$ZSN
cv.out.lasso <- cv.glmnet(x, y, alpha = 1, family = "binomial", nfolds = nrow(MI.work))
plot(cv.out.lasso)
lambda.best.lasso <- cv.out.lasso$lambda.min
# fit lasso regression on the dataset
mod.lasso <- glmnet(x, y, alpha = 1, lambda = lambda.best.lasso, family = "binomial")
mod.lasso$beta # 24 predictors selected by lasso
# Compute AIC values of lasso model
sAIC(x = x, y = y, coef(mod.lasso), family = "binomial") # 1031.471
# compute LOOCV prediction error rate and test error rate of Lasso regression
lasso.test.error <- rep(0, n = 1:nrow(MI.work))
lasso.test.pred <- rep(0, n = 1:nrow(MI.work))
lasso.pred.error <- rep(0, n = 1:nrow(MI.work))
lasso.auc <- rep(0, n = 1:nrow(MI.work))
for (i in 1:nrow(MI.work)){
  x.train <- x[-i, ]
  y.train <- y[-i]
  x.test <- as.matrix(x[i, ])
  y.test <- y[i]
  
  mod <- glmnet(x.train, y.train, alpha = 1, lambda = lambda.best.lasso, family = "binomial")
  
  lasso.pred.error[i] <- mean(predict(mod, newx = x.train, s = lambda.best.lasso, type = "class") != y.train)
  lasso.test.pred[i] <- predict(mod, newx = t(data.matrix(x.test)), s = lambda.best.lasso, type = "class")
  lasso.test.error[i] <- ifelse( lasso.test.pred[i] != y.test, 1, 0)
  
  predlasso <- prediction(predict(mod, data.matrix(x.train), type = "response"), y.train)
  stats <- performance(predlasso, 'tpr', 'fpr')
  
  auc <- performance(predlasso, measure = "auc")
  lasso.auc[i] <- auc@y.values[[1]]
}
mean(lasso.pred.error) # 0.197
mean(lasso.test.error) # 0.204
mean(lasso.auc) # 0.779
```


Ridge regression
```{r}
set.seed(1)
cv.out.ridge <- cv.glmnet(x, y, alpha = 0, family = "binomial", nfolds = nrow(MI.work))
plot(cv.out.ridge)
lambda.best.ridge <- cv.out.ridge$lambda.min
# fit ridge regression on the dataset
mod.ridge <- glmnet(x, y, alpha = 0, lambda = lambda.best.ridge, family = "binomial") # 88 predictors by ridge
mod.ridge$beta
# Compute AIC values of ridge model
sAIC(x = x, y = y, coef(mod.ridge), family = "binomial") 
# compute LOOCV test error rate of ridge regression
ridge.pred.error <- rep(0, n = 1:nrow(MI.work))
ridge.test.error <- rep(0, n = 1:nrow(MI.work))
ridge.test.pred <- rep(0, n = 1:nrow(MI.work))
ridge.auc <- rep(0, n = 1:nrow(MI.work))
for (i in 1:nrow(MI.work)){
  x.train <- x[-i, ]
  y.train <- y[-i]
  x.test <- as.matrix(x[i, ])
  y.test <- y[i]
  
  mod <- glmnet(x.train, y.train, alpha = 0, lambda = lambda.best.ridge, family = "binomial")
  
  ridge.pred.error[i] <- mean(predict(mod, newx = x.train, s = lambda.best.ridge, type = "class") != y.train)
  ridge.test.pred[i] <- predict(mod, newx = t(data.matrix(x.test)), s = lambda.best.ridge, type = "class")
  ridge.test.error[i] <- ifelse(ridge.test.pred[i] != y.test, 1, 0)
  
   predridge <- prediction(predict(mod, data.matrix(x.train), type = "response"), y.train)
   stats <- performance(predridge, 'tpr', 'fpr')
   
   auc <- performance(predridge, measure = "auc")
   ridge.auc[i] <- auc@y.values[[1]]
}
mean(ridge.pred.error) # 0.199
mean(ridge.test.error) # 0.221
mean(ridge.auc) # 0.807
```


Lasso regression + prescreen
```{r}
# select best lambda via LOOCV 
set.seed(1)
x.s <- model.matrix(ZSN ~., prescreen(MI.work))[, -1]
y.s <- prescreen(MI.work)$ZSN
cv.out.lasso.s <- cv.glmnet(x.s, y.s, alpha = 1, family = "binomial", nfolds = nrow(MI.work))
plot(cv.out.lasso.s)
lambda.best.lasso.s <- cv.out.lasso.s$lambda.min
# fit lasso regression on the dataset
mod.lasso.s <- glmnet(x.s, y.s, alpha = 1, lambda = lambda.best.lasso.s, family = "binomial") # 24 predictors
mod.lasso.s$beta
# Compute AIC values of lasso model
sAIC(x = x.s, y = y.s, coef(mod.lasso.s), family = "binomial") # 1022.898
# compute LOOCV prediction error rate and test error rate of Lasso regression
lasso.test.error <- rep(0, n = 1:nrow(MI.work))
lasso.pred.error <- rep(0, n = 1:nrow(MI.work))
lasso.test.pred <- rep(0, n = 1:nrow(MI.work))
lasso.auc <- rep(0, n = 1:nrow(MI.work))
# x.pred <- c(1:1064)
# y.pred <- c(1:1064)
for (i in 1:nrow(MI.work)){
        
        work.dat <- MI.work[-i,]
        train.dat <- MI.work[i, ]
        
        x.train <- prescreen(work.dat)[, -1]
        y.train <- prescreen(work.dat)[, 1]
        
        x.test <- train.dat[, names(prescreen(work.dat))][, -1]
        y.test <- train.dat[, names(prescreen(work.dat))][, 1]
        
        mod <- glmnet(x.train, y.train, alpha = 1, lambda = lambda.best.lasso, family = "binomial")
        
        lasso.pred.error[i] <- mean(predict(mod, newx = data.matrix(x.train), s = lambda.best.lasso, type = "class") != y.train)
        lasso.test.pred[i] <- predict(mod, newx = data.matrix(x.test), s = lambda.best.lasso, type = "class")
        lasso.test.error[i] <- ifelse( lasso.test.pred[i] != y.test, 1, 0)
        predlasso <- prediction(predict(mod, data.matrix(x.train), type = "response"), y.train)
        stats <- performance(predlasso, 'tpr', 'fpr')
        #x.pred <- cbind(x.pred, stats@x.values[[1]])
        #y.pred <- cbind(y.pred, stats@y.values[[1]])
        auc <- performance(predlasso, measure = "auc")
        lasso.auc[i] <- auc@y.values[[1]]
}
mean(lasso.pred.error) # 0.237
mean(lasso.test.error) # 0.266
mean(lasso.auc) #0.7598
```

Ridge regression + prescreen
```{r}
set.seed(1)
cv.out.ridge.s <- cv.glmnet(x.s, y.s, alpha = 0, family = "binomial", nfolds = nrow(MI.work))
plot(cv.out.ridge.s)
lambda.best.ridge.s <- cv.out.ridge.s$lambda.min
# fit ridge regression on the dataset
mod.ridge.s <- glmnet(x.s, y.s, alpha = 0, lambda = lambda.best.ridge.s, family = "binomial") # 32 predictors by ridge
mod.ridge.s$beta
# Compute AIC values of lasso model
sAIC(x = x.s, y = y.s, coef(mod.ridge.s), family = "binomial") # 1042.574
# compute LOOCV test error rate of ridge regression
ridge.pred.error <- rep(0, n = 1:nrow(MI.work))
ridge.test.error <- rep(0, n = 1:nrow(MI.work))
ridge.test.pred <- rep(0, n = 1:nrow(MI.work))
ridge.auc <- rep(0, n = 1:nrow(MI.work))
for (i in 1:nrow(MI.work)){
        work.dat <- MI.work[-i,]
        train.dat <- MI.work[i, ]
        
        x.train <- prescreen(work.dat)[, -1]
        y.train <- prescreen(work.dat)[, 1]
        
        x.test <- train.dat[, names(prescreen(work.dat))][, -1]
        y.test <- train.dat[, names(prescreen(work.dat))][, 1]
        
        mod.r <- glmnet(x.train, y.train, alpha = 1, lambda = lambda.best.ridge, family = "binomial")
        
        ridge.pred.error[i] <- mean(predict(mod.r, newx = data.matrix(x.train), s = lambda.best.ridge, type = "class") != y.train)
        ridge.test.pred[i] <- predict(mod.r, newx = data.matrix(x.test), s = lambda.best.ridge, type = "class")
        ridge.test.error[i] <- ifelse(ridge.test.pred[i] != y.test, 1, 0)
        
        predridge <- prediction(predict(mod.r, data.matrix(x.train), type = "response"), y.train)
        stats <- performance(predridge, 'tpr', 'fpr')
        #x.pred <- cbind(x.pred, stats@x.values[[1]])
        #y.pred <- cbind(y.pred, stats@y.values[[1]])
        auc <- performance(predridge, measure = "auc")
        ridge.auc[i] <- auc@y.values[[1]]
}
mean(ridge.pred.error) # 0.253
mean(ridge.test.error)# 0.2495
mean(ridge.auc) # 0.7025
```


```{r}
# compare lasso and ridge test error and training error
pglm.table0 <- data.frame(Type = c("Lasso", "Ridge"), Testing_error = c("0.255", "0.253"), Training_error = c("0.249", "0.248"))
pglm.table <- data.frame(t(pglm.table0[, 2:3]))
colnames(pglm.table) <- pglm.table0[, 1]
```

```{r}
kable(pglm.table) %>% kable_styling(font_size = 14, full_width = FALSE) %>% save_kable("error_comparison.pdf")
```


```{r}
tab.lasso <- tidy(coef(mod.lasso))[, -2]
tab.ridge <- tidy(coef(mod.ridge))[, -2]
seperate <- left_join(tab.ridge, tab.lasso, by = "row")
colnames(seperate) <- c("Coefficients", "Ridge estimates", "Lasso estimates")
seperate
share <- left_join(tab.lasso, tab.ridge, by = "row")
colnames(share) <- c("Coefficients", "Lasso estimates", "Ridge estimates")
share
```

```{r}
#opts_chunk$set(echo = TRUE)
kable(seperate, caption = "Lasso and Ridge coefficient estimates") %>% kable_styling(font_size = 10, full_width = F) %>% save_kable("seperate.pdf")
```

```{r}
kable(share, caption = "Lasso and Ridge coefficient estimates") %>% kable_styling(font_size = 10, full_width = F) %>% save_kable("share.pdf")
```


```{r}
# ROC curve of lasso and ridge regression
predlasso <- prediction(predict(mod.lasso, x, type = "response"), MI.work$ZSN)
stats5 <- performance(predlasso, 'tpr', 'fpr')
predridge <- prediction(predict(mod.ridge, x, type = "response"), MI.work$ZSN)
stats6 <- performance(predridge, 'tpr', 'fpr')
plot(stats5@x.values[[1]], stats5@y.values[[1]], type='s', 
     ylab=stats5@y.name, xlab=stats5@x.name, col='red', lwd=2, lty=2)
lines(stats6@x.values[[1]], stats6@y.values[[1]], type='s', col='blue', lwd=2, )
#lines(stats2@x.values[[1]], stats2@y.values[[1]], type='s', col='green', lwd=2, )
abline(0,1, col='gray')
legend('bottomright', c("lasso AUC: 0.779", "ridge AUC: 0.807"), col=c('red','blue'), 
       lwd=c(2,1), lty=c(2,1), cex=.9, bty='n')
```


# KNN
```{r}
library(class)
# use LOOCV to determine the best number of K
x.matrix <- as.matrix(MI.work[, -95])
k <- c(1:50)
pred <- rep(0, nrow(MI.work))
pred.error <- rep(0, length(k))
knn.pred <- rep(0, nrow(MI.work))
for (j in 1:length(k)){
        for (i in 1:nrow(MI.work)){
                # set up training set and testing set
                train.x <- x.matrix[-i, ]
                test.x <- x.matrix[i, ]
                train.ZSN <- MI.work$ZSN[-i]
                test.ZSN <- MI.work$ZSN[i]
                
                set.seed(1)
                knn.pred[i] <-  knn(train.x, test.x, train.ZSN, k = k[j])
                pred[i] <- ifelse(knn.pred[i] != test.ZSN, 1, 0)
        }
        
        pred.error[j] <- mean(pred)
}
which.min(pred.error) #27
knn.output <- as.data.frame(cbind(pred.error, k))
ggplot(knn.output, aes(y = pred.error, x = k)) + geom_point() + labs(title = "KNN test error rate by number of k", x = "k", y = "LOOCV test error rate") + geom_vline(xintercept = which.min(pred.error), color = "blue", linetype = "dotted") + scale_x_continuous(breaks = c(0, 25, 27, 50)) 
```


```{r}
knn.train.error <- rep(0, nrow(MI.work))
knn.test.error <- rep(0, nrow(MI.work))
knn.test <- rep(0, nrow(MI.work))
for (i in 1:nrow(MI.work)){
        # set up training set and testing set
        train.x <- x.matrix[-i, ]
        test.x <- x.matrix[i, ]
        train.ZSN <- MI.work$ZSN[-i]
        test.ZSN <- MI.work$ZSN[i]
                
        set.seed(1)
                
        # compute training error rate
        knn.train <- knn(train.x, train.x, train.ZSN, k = 27)
        knn.train.error[i] <- mean(knn.train != train.ZSN)
                
        # compute testing error rate
        knn.test[i] <-  knn(train.x, test.x, train.ZSN, k = 27)
        knn.test.error[i] <- ifelse(knn.test[i] != test.ZSN, 1, 0)
}
mean(knn.train.error)
mean(knn.test.error)
```

# Coefficient comparision between backward and lasso
```{r}
coefb <- coef(glm.fitb)
tab.coefb <- data.frame(list(names = names(coefb), coefs = coefb))
colnames(tab.coefb) <- c("Coefficients", "Backward estimates")
tab.lasso <- tidy(coef(mod.lasso))[, -2]
colnames(tab.lasso) <- c("Coefficients", "Lasso estimates")
back.lasso.full <- full_join(tab.coefb, tab.lasso, by = "Coefficients")[, -4]
kable(back.lasso.full, caption = "Table 4. Coefficient estimates form backward selection and lasso regression") %>% kable_styling(font_size = 10, full_width = F)
```


```{r}
kable(back.lasso.full, caption = "coefficient estimates form backward selection and lasso regression") %>% kable_styling(font_size = 10, full_width = F) %>% save_kable("backlassofull.pdf")
```

```{r}
kable(back.lasso.share, caption = "shared coefficients of backward selection and lasso regression") %>% kable_styling(font_size = 10, full_width = F) %>% save_kable("backlassoshare.pdf")
```


